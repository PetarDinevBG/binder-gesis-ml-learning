{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ICWSM-2020-Twitter-Inappropriate-Speech/hatespeech_text_label_vote_RESTRICTED_100K.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = []\n",
    "label = []\n",
    "with open(path) as fi:\n",
    "    data = csv.reader(fi, delimiter='\\t')\n",
    "    for row in data:\n",
    "        tweet.append(row[0])\n",
    "        label.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet) == len(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count occurences of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'spam': 14030, 'abusive': 27150, 'normal': 53851, 'hateful': 4965})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweet, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reports(results, report_name, y_true, y_test_predicted):\n",
    "    acc = metrics.accuracy_score(y_true, y_test_predicted, normalize=True, sample_weight=None)\n",
    "    report = metrics.classification_report(y_test, y_test_predicted)\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:]:\n",
    "        row = {}\n",
    "        row_data = line.split('    ')\n",
    "        row_data = [item for item in row_data if len(item) > 1]\n",
    "        if len(row_data) > 2:\n",
    "            row['classifier'] = report_name\n",
    "            row['class'] = row_data[0].strip()\n",
    "            row['precision'] = float(row_data[1])\n",
    "            row['recall'] = float(row_data[2])\n",
    "            row['f1_score'] = float(row_data[3])\n",
    "            row['support'] = float(row_data[4])\n",
    "            row['accuracy'] = float(acc)\n",
    "            report_data.append(row)\n",
    "        \n",
    "    results = results.append(pd.DataFrame(report_data), sort=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_pipeline(pipeline, report_name, results, \n",
    "                 y_train, X_train,\n",
    "                y_test, X_test):\n",
    "    \n",
    "    text_clf = Pipeline(pipeline)\n",
    "    text_clf = text_clf.fit(X_train, y_train)\n",
    "    y_test_predicted = text_clf.predict(X_test)\n",
    "    results = get_reports(results, report_name, y_test, y_test_predicted)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    'LogisticRegression': LogisticRegression(), \n",
    "    'SGDClassifier':  SGDClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'GaussianNB' : GaussianNB(),\n",
    "    'BernoulliNB' : BernoulliNB(),\n",
    "    'MultinomialNB' : MultinomialNB(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for algo_name, algo in algos.items():\n",
    "    print(algo_name)\n",
    "    try:\n",
    "        pipeline = [('vect', CountVectorizer()),\n",
    "                    ('clf', algo)]\n",
    "        report_name = algo_name + '_Count'\n",
    "        \n",
    "        results = run_pipeline(pipeline, report_name, results, y_train, X_train, y_test, X_test) \n",
    "\n",
    "        pipeline = [('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', algo)]\n",
    "        \n",
    "        report_name = algo_name + '_tfidf'\n",
    "        results = run_pipeline(pipeline, report_name, results, y_train, X_train, y_test, X_test) \n",
    "    except Exception as e:\n",
    "        results = results.append(pd.DataFrame([{'classifier': algo_name, 'error': str(e)},]), sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sorted results per classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier                    class    \n",
       "LogisticRegression_BoW        macro avg    0.73\n",
       "SGDClassifier_BoW             macro avg    0.73\n",
       "LogisticRegression_BoW_tfidf  macro avg    0.75\n",
       "SGDClassifier_BoW_tfidf       macro avg    0.78\n",
       "Name: precision, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the micro average < macro average one:  misclassification in the most populated labels\n",
    "# if the micro average > macro average one:  misclassification in the least populated labels\n",
    "results[results['class'] == 'weighted avg'].groupby(['classifier', 'class'])['precision'].max().sort_values()\n",
    "results[results['class'] == 'macro avg'].groupby(['classifier', 'class'])['precision'].max().sort_values()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
